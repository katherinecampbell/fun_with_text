{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL-divergence between d1 and d2: 3.2643207211140726\n",
      "KL-divergence between d2 and d1: 2.8584931462421044\n",
      "KL-divergence between d1 and d3: 7.048735294829035\n",
      "KL-divergence between d2 and d3: 7.181256314545033\n"
     ]
    }
   ],
   "source": [
    "# Manos Tsagkias' program for computing Kullback-Liebler Divergence\n",
    "# Using the Migge (2003) smoothening backoff\n",
    "# see http://staff.science.uva.nl/~tsagias/?s=kullback\n",
    "# updated for Python3 by Mark Keane 30-June-2014\n",
    "\n",
    "import re, math, collections\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def tokenize(_str):\n",
    "\n",
    "    stopwords = ['and', 'for', 'if', 'too', 'as', 'the', 'then', 'be', 'is', 'are', 'will', 'in', 'it', 'to', 'that']\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n",
    "        m = m.group(1).lower()\n",
    "        if len(m) < 2: continue\n",
    "        if m in stopwords: continue\n",
    "        tokens[m] += 1\n",
    "    return tokens\n",
    "#end of tokenize\n",
    "\n",
    "def kldiv(_s, _t):\n",
    "    if (len(_s) == 0):\n",
    "        return 1e33\n",
    "    if (len(_t) == 0):\n",
    "        return 1e33\n",
    "    ssum = 0. + sum(_s.values())\n",
    "    slen = len(_s)\n",
    "    tsum = 0. + sum(_t.values())\n",
    "    tlen = len(_t)\n",
    "    vocabdiff = set(_s.keys()).difference(set(_t.keys()))\n",
    "    lenvocabdiff = len(vocabdiff)\n",
    "\n",
    "    #print(\"_s: %s\" % _s)\n",
    "    #print(\"_t: %s\" % _t)\n",
    "    #print(\"%s\" % vocabdiff)\n",
    "\n",
    "    \"\"\" epsilon \"\"\"\n",
    "    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001\n",
    "    \n",
    "    \"\"\" gamma \"\"\"\n",
    "    gamma = 1 - lenvocabdiff * epsilon\n",
    "    \n",
    "    \"\"\" Check if distribution probabilities sum to 1\"\"\"\n",
    "    sc = sum([v/ssum for v in _s.values()])  \n",
    "    st = sum([v/tsum for v in _t.values()]) \n",
    "    \n",
    "    if sc < 9e-6:\n",
    "        print(\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print(\"*** ERROR: sc does not sum up to 1. Bailing out ..\")\n",
    "        sys.exit(2)\n",
    "    if st < 9e-6:\n",
    "        print(\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print(\"*** ERROR: st does not sum up to 1. Bailing out ..\")\n",
    "        sys .exit(2)\n",
    "\n",
    "    div = 0.\n",
    "    for t, v in _s.items(): \n",
    "        pts = v / ssum\n",
    "        ptt = epsilon\n",
    "        if t in _t:\n",
    "            ptt = gamma * (_t[t] / tsum)\n",
    "            \n",
    "        ckl = (pts - ptt) * math.log(pts / ptt)\n",
    "\n",
    "        div +=  ckl\n",
    "    return div\n",
    "\n",
    "#end of kldiv\n",
    "\n",
    "d1 = \"\"\"John fell down. Harry fell as-well down by the stream. The sun shone before it went down. Mary was fine.\"\"\"\n",
    "\n",
    "d2 = \"\"\"Bill fell down. Jeff fell down too down by the river. The sun shone until it sunk down.  Belinda was ill.\"\"\"\n",
    "\n",
    "#d3 = \"\"\"Katherine ate cake yesterday. Marian and Amie ate cake as well. The cake was a chocolate cake. All three girls ate cake until they were all so full they couldnâ€™t move. The next day, all three girls felt very sick\"\"\"\n",
    "\n",
    "\n",
    "print(\"KL-divergence between d1 and d2:\", kldiv(tokenize(d1), tokenize(d2)))\n",
    "print(\"KL-divergence between d2 and d1:\", kldiv(tokenize(d2), tokenize(d1)))\n",
    "#print(\"KL-divergence between d1 and d3:\", kldiv(tokenize(d1), tokenize(d3)))\n",
    "#print(\"KL-divergence between d2 and d3:\", kldiv(tokenize(d2), tokenize(d3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
