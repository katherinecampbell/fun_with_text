{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### a.) Get Da Data\n",
    "    Data in CSV format retrieved from Kaggle - real tweets about the most recent presidential inauguration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('inaugurationA.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "#take the first ten examples\n",
    "df = pd.read_csv('inaugurationA.csv', encoding=result['encoding'], nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @9gagtv: President Donald Trump Accidentall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @SpcStevens: This makes it much easier. #In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Claire_Phipps @nwg83 so what youre saying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @JahovasWitniss: Wait... what!?\\r #Inaugura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @3lectric5heep: BREAKING: Here's The Full L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @DanJGross: Crowd count is in:\\r Trump 2017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @FunnyVines: The 45th President of The Unit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @JCRaskaus: BREAKING: Here's The Full List ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @LessGovMoreFun: BREAKING: Here's The Full ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @WBLZSportsChica: 1 million #Patriots fans ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  RT @9gagtv: President Donald Trump Accidentall...\n",
       "1  RT @SpcStevens: This makes it much easier. #In...\n",
       "2  RT @Claire_Phipps @nwg83 so what youre saying ...\n",
       "3  RT @JahovasWitniss: Wait... what!?\\r #Inaugura...\n",
       "4  RT @3lectric5heep: BREAKING: Here's The Full L...\n",
       "5  RT @DanJGross: Crowd count is in:\\r Trump 2017...\n",
       "6  RT @FunnyVines: The 45th President of The Unit...\n",
       "7  RT @JCRaskaus: BREAKING: Here's The Full List ...\n",
       "8  RT @LessGovMoreFun: BREAKING: Here's The Full ...\n",
       "9  RT @WBLZSportsChica: 1 million #Patriots fans ..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = df.iloc[:,1]\n",
    "pd.DataFrame(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data.to_csv('practical_4_q1.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have the tweet data in a txt file which can then be manipulated in python and R. However, it must be cleaned first. To do this, each text item must be investigated. Luckily, all tweets in question begin with RT so splitting them into different text items on the RT is simple. It would have been a different code if there hadn't been such an easy text-item division marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stopwords used were from the nltk.corpus library. Useless characters were also marked. Twitter handles, marked by the '@' are removed later in this code because they don't carry much meaning, they are merely names, and thus mean nothing other than to identify the person in question.  Here, the shared topic of the tweets is not named based (e.g. all tweets @realdonaldtrump) so handles were deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "List = open(\"practical_4_q1.txt\").read()\n",
    "List = List.split('RT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of URLs\n",
    "new_list=[]\n",
    "for sentence in List:\n",
    "    URLless_string = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', sentence)\n",
    "    sentence = URLless_string\n",
    "    new_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of stopwords and useless characters\n",
    "useless_char =['\\n','[',']','\\'','\\\"',':','(',')',':','#','.','1','2','3','4','5','6','7','8','9','0']\n",
    "stop = set(stopwords.words('english'))\n",
    "txt = []\n",
    "for sent in new_list:\n",
    "    for ch in useless_char:\n",
    "        sent=sent.replace(ch,\" \")\n",
    "    no_stops = [i for i in sent.lower().split() if i not in stop and i != 'rt'] #term rt unecessary. Might change results.\n",
    "    txt.append(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of twitter handles because they aren't real words and are only useful to twitter - might change results\n",
    "cleaned_tweets = []\n",
    "for sentence in txt:\n",
    "    new_list = [word for word in sentence if not word.startswith('@')]\n",
    "    j = \" \";\n",
    "    no_handles = j.join(new_list)\n",
    "    cleaned_tweets.append(no_handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of empty strings in the list\n",
    "cleaned_tweets = list(filter(None, cleaned_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('new_corpus.txt','w') as f:\n",
    "    for tweet in cleaned_tweets:\n",
    "        f.write(tweet)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, write these new clean tweets to new text files that may be used for the rest of the lab. The trick is to save each tweet to it's own file. Code below from Stackoverflow user Martin Thoma https://stackoverflow.com/questions/17984809/how-do-i-create-a-incrementing-filename-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nonexistant_path(fname_path):\n",
    "    \"\"\"\n",
    "    Get the path to a filename which does not exist by incrementing path.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> get_nonexistant_path('/etc/issue')\n",
    "    '/etc/issue-1'\n",
    "    >>> get_nonexistant_path('whatever/1337bla.py')\n",
    "    'whatever/1337bla.py'\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fname_path):\n",
    "        return fname_path\n",
    "    filename, file_extension = os.path.splitext(fname_path)\n",
    "    i = 1\n",
    "    new_fname = \"{}-{}{}\".format(filename, i, file_extension)\n",
    "    while os.path.exists(new_fname):\n",
    "        i += 1\n",
    "        new_fname = \"{}-{}{}\".format(filename, i, file_extension)\n",
    "    return new_fname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in cleaned_tweets:   \n",
    "    fname = get_nonexistant_path(\"cleanedTweets.txt\")\n",
    "    text_file = open(fname, \"w\")\n",
    "    text_file.write(tweet)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
